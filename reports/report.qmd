---
title: Homework 1 - Open Source Tools
author:
    - name: Victoria Castagnola
      email: castagnola.victoria@gmail.com or castagnolave@vcu.edu
date: 8/31/2025
---

[Link to project repository](https://github.com/cmsc-vcu/cmsc408-fa2025-hw1-castagnolave)

This report summarizes the main categories of data engineering tool. Additionally, it goes further into depth on why each is important, what it does, and how it is different from OLTP. There are some subcategories that have been picked out due to my (the author's) interest in them, with further descriptions following. At the end, there is a reflection of the assignment and the process of writing this very report.

# Open Source Data Engineering Tools

Author Alireza Sadeghi offers a nice overview of the [2025 data engineering landscape](https://www.pracdata.io/p/open-source-data-engineering-landscape-2025) in the on-line web site [Practical Data Engineering Substack](https://practicaldataengineering.substack.com/).

![](assets/tools-2025.webp)

# Major Categories

Mr. Sadeghi proposals nine major tools categories.

## Storage Systems

Storage systems are foundational components in data engineering that handle
the storage and retrieval of data. These systems can include traditional
databases, distributed storage solutions, and modern cloud-based storage 
options. They provide scalable, reliable, and secure environments to store 
structured, semi-structured, and unstructured data, ensuring data is
accessible for processing, analysis, and other operations.

## Data Lake Platform

Data Lake platforms are designed to store vast amounts of raw data in its native format until it is needed. Unlike traditional databases, data lakes can store structured, semi-structured, and unstructured data, providing a flexible and scalable storage solution. These platforms support the integration of multiple data sources, making it easier to perform big data analytics and machine learning on large datasets.

## Data Integration

Data Integration platforms have been popularized because many companies do not need to access big data as much as they may have previously estimated, thus single-node solutions are often enough for their needs. These platforms enable movement and transformation of data between sources, so it can stay consistent and accurate. They support Extract, Transform, Load (ETL) processes, API integrations, and real-time data synchronization, among others. There are many different options, making them great for modern data flow needs.

## Data Processing and Computation

Data processing and computation, also called data ingestion, is the process of 
turning raw data into a format that's easy to work with for analysis and decision-making. 
This includes tasks like cleaning up messy data, combining it, summarizing it, and adding relevant details to make it more meaningful. Modern tools often handle big datasets by 
breaking the work down into pieces and processing them from there. These tools are 
key for tasks like batch processing, real-time data streams, and digging into data to 
find useful insights. This way, companies and people can use the data.

## Workflow and DataOps

Workflow and DataOps encompass the whole process, from data collection to processing, as well as any human contribution points. These tools help move data from one place to another while making sure it's correct and ready to use. They also make it easier for teams to work together by showing how data flows, keeping track of changes, and making sure processes can be repeated. DataOps applies ideas from agile and DevOps to improve how data is handled and shared.

## Data Infrastructure and Monitoring

Data infrastructure is the bones of it all. It includes all of the physical 
and virtual components that support the storage, processing, and movement of data. 
This category covers servers, networking equipment, cloud services, and 
databases that form the backbone of any data  system. Another example of infrastructure would be the highways and other roads of America. Proper data 
infrastructure ensures that data is accessible, secure, and scalable, enabling 
organizations to handle increasing amounts of data efficiently. Monitoring tools 
are also a very importat part of data infrastructure because they help track system 
performance, identify bottlenecks, and ensure reliability. Without a solid data 
infrastructure, even the most advanced data engineering tools would struggle to 
function. 

## ML/AI Platform

ML platforms offer tools for building, training, and deploying machine learning models, covering tasks like data preparation, model training, parameter optimization, and deployment workflows. These platforms often use cloud-based solutions for scalability and support teamwork through features like version control and experiment tracking. Open-source examples include PyTorch and TensorFlow, enabling organizations to make informed, data-driven decisions.

## Metadata Management

Metadata management is the process of organizing metadata to ensure that it is accessible, accurate, and compliant with applicable regulations. Metadata, often referred to as "data about data," provides context and meaning to raw data, making it easier to understand, go through, and utilize. Effective metadata management is crucial in larger databases, where the sheer volume of data can make it challenging to maintain organization and compliance. Tools in this category help catalog, govern, and track metadata, enabling better data discovery, lineage, and quality control.

## Analytics and Visualization

Analytics and visualization tools help make sense of data and share insights. Analytics tools analyze data to find patterns, trends, and useful information. Visualization tools turn this data into charts, graphs, and dashboards that are easy to understand.

Modern platforms often combine analytics and visualization, making it easier to analyze and present data in one place. These tools are used in many areas, like business and research, to find and explain important information. Examples of open-source tools include Apache Superset, Metabase, and Plotly.

# Digging into the details

In the following sections I identify three subcategories of data engineering tools of greatest interest to me.

## Event Hubs

Event Hubs is a data ingestion service by Microsoft Azure. It is designed to be low-latency and has the ability to process millions of events per second, from several sources. 


### Characteristics of Event Hubs
1. **Stream Analytics:**
   - Data is processed in real time. Event Hubs takes in raw streaming data and Stream Analytics looks at it and can make a decision in that moment, like send a notification or log the data.
   - Data can be output to Power BI, Azure Data Lake, Blob Storage, or SQL Database. There are many ways to output this data.
2. **Azure Data Lake & Blob Storage:**
   - Stream Analytics is able to work to store results through Blob Storage or Data Lakes. This is useful for long-term storage, or when something needs to be reported at a later date.
   - Blob storage tends to work better for smaller-scale analytics, as it is simple and cheap. It can also be used to archive or backup.
   - Data Lake is good for large-scale data, 
   especially for advanced analytics. It is hierarchal in structure, unlike blob storage, so it is especially good for those larger real-time cases in terms of relation to Azure Stream Analytics.


### Why It's Important
1. **Real-Time Processing**
   - Because ASA processes data in real time, it can be used for critical events like equipment failures and fraud detection.
   - Businesses, governments, and even international organizations can all use this for scenarios like real-world emergencies on any scale.
2. **Simplicity**
   - There is no need for complex programming languages to use ASA.
   - The queries are akin to SQL, making ASA accessible for more organizations.


### Azure Stream Analytics versus OLTP
   - ASA is built for real-time data analysis, while OLTP is built for real-time business transactions.
   - Generally, OLTP is a little faster, as ASA is often handling larger queries in real time.
   - ASA is not designed to have many users on the system, while OLTP is built with that in mind.
   - ASA does not store anything directly, or change it, but OLTP often will, as it is built to update, store, delete, and even add data.
   - OLTP is used for things like e-commerce checkout and inventory updates.
   - ASA is used for things like detecting fraud and live dashboards displaying real-time data.


### Why Is It Interesting?
     The author of the article mentions that there is a BYOC (Bring Your Own Cloud) deployment strategy, which isn't something I am used to seeing. I think that this definitely changes how I am used to thinking about things like google drive and iCloud, where those are stored on their cloud, but this is a totally different ballgame.


## MPP Query Engines

Massively Parallel Processing Query Engines are capable of running large queries by splitting them up into smaller tasks. These smaller tasks are then run in parallel across many machines and then the results are combines to get a quick answer.


### Characteristics of MPP Query Engines
1. **Speed**
   MPP allows for even large queries to gather results quickly.
2. **Scale**
   MPP can handle very large sets of data.
3. **Parallelism**
   MPP uses multiple processors/devices at once to get its results.
4. **Workload Suitability**
   MPP are not generally designed for small workloads, and can be very expensive.


### Why It's Important
   - MPP are able to scale quickly as the need to handle more and more data increases.
   - They can analyze extremely large amounts of data very quickly.
   - They work quickly and are compatible with many other data engineering tools.


### MPP versus OLTP
1. **Scale**
   - MPP handle terabytes to petabytes of info at a time!
   - OLTP handle very small transactions, but they are much more frequent.
2. **Concurrency**
   - MPP has few users, but the queries are much meatier and heavier in substance. 
   - OLTP has many users, and supports fast read/writes.


### Why Is It Interesting?
   The author of the article states that there have not been many changes to this aspect of data engineering technology in the past year. Despite this, I found MPP very interesting because of their extremely large caliber. One cannot even fathom the amount of data in a petabyte, and MPP work on that scale every day.


## Python Processing Frameworks

   Python Processing Frameworks are libraries/tools that help users process large amounts of data, run data workflows, and handle data streaming in real time. They can also help with scaling processing to several machines.


### Examples of Python Processing Frameworks

1. **Apache Spark**
   - Apache Spark is a distributed big data engine.
   - It is generally best for large batch processing or streaming processing. 
   - It is often used in machine learning.

2. **Celery**
   - Celery is a distributed task queue which is good for task execution that is asynchronous.
   - It is good for background things as opposed to analytics.
   - It is used for things like sending emails.

### Why It's Important
PPFs allow people to use data tools at small, medium, and large scales. A solo web developer would not have the need for something like Amazon Athena, or a PPF like Apache Spark, but they may find great use in Celery.


### PPF versus OLTP
   - PPF are more of a transformation and analytical set of tools. They can be used for many different scalable things, depending on the type.
   - OLTP are much more traditional, and are better for real-time transactions like payments or a username update.

### Why Is It Interesting?
   I found the idea of PPFs interesting because there are so many different ones. I only added a few but I read about several more, and the possibilities seem almost endless. Additionally, the names of some of them that I skimmed over were so silly to me that I just had to learn more about what they were.


# Reflection

* I really liked being able to pick my own sub-sections of data engineering tools to learn more about. It definitely made me curious to learn about the others that I did not research as much, especially since the field is always changing.

* The project, as described, was very meaty. It made it difficult and overwhelming to start, but once I had laid more of a skeleton out than was already given (thank you!), I had no problem.

* I did not know how long it was going to take me. Despite the warnings, I thought it would not take me that long to complete this project. Additionally, I actually had fun for the first time in a while researching without just making AI assist me with writing.

* Next time, I would definitely start way sooner so I do not turn it in late again. Additionally, I would probably add more versions as my setup was a bit shaky initially so my version control is a little off.


# README

A quality README is an important part of EVERY project. Using the Quarto *include* command we're including a copy of your README in the project report so that a human can evaluate it.

Make sure that you edit the README so that it's explanatory!  Note that you don't need a readme within the *reports* folder for this assignment. We're only
focused on the root *README.md*.

[Here is some info](https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/) on how to write a good README!

::: {style="background:lightgray; margin-left:20px; border-top: 3px solid black; border-bottom: 3px solid black; padding-left:20px; padding-right:20px"}
{{< include ../README.md >}}
:::
