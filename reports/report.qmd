---
title: Homework 1 - Open Source Tools
author:
    - name: Victoria Castagnola
      email: castagnola.victoria@gmail.com or castagnolave@vcu.edu
date: 8/31/2025

## Useful references:
# - Basic Markdown: https://quarto.org/docs/authoring/markdown-basics.html
# - Quarto figures: https://quarto.org/docs/authoring/figures.html
# - HTML document basics: https://quarto.org/docs/output-formats/html-basics.html
# - Quarto guide: https://quarto.org/docs/guide/
# - VS Code and Quarto: https://quarto.org/docs/tools/vscode.html
#   (RTFM and GET THE EXTENSION!!!!)
---

[Link to project repository](https://github.com/cmsc-vcu/cmsc408-fa2025-hw1-castagnolave)

(Add an intro paragraph here.  What is the purpose of this report?)

# Open Source Data Engineering Tools

Author Alireza Sadeghi offers a nice overview of the [2025 data engineering landscape](https://www.pracdata.io/p/open-source-data-engineering-landscape-2025) in the on-line web site [Practical Data Engineering Substack](https://practicaldataengineering.substack.com/).

![](assets/tools-2025.webp)

# Major Categories

Mr. Sadeghi proposals nine major tools categories.

## Storage Systems

Storage systems are foundational components in data engineering that handle
the storage and retrieval of data. These systems can include traditional
databases, distributed storage solutions, and modern cloud-based storage 
options. They provide scalable, reliable, and secure environments to store 
structured, semi-structured, and unstructured data, ensuring data is
accessible for processing, analysis, and other operations.

## Data Lake Platform

Data Lake platforms are designed to store vast amounts of raw data in its native format until it is needed. Unlike traditional databases, data lakes can store structured, semi-structured, and unstructured data, providing a flexible and scalable storage solution. These platforms support the integration of multiple data sources, making it easier to perform big data analytics and machine learning on large datasets.

## Data Integration

Data Integration platforms have been popularized because many companies do not need to access big data as much as they may have previously estimated, thus single-node solutions are often enough for their needs. These platforms enable movement and transformation of data between sources, so it can stay consistent and accurate. They support Extract, Transform, Load (ETL) processes, API integrations, and real-time data synchronization, among others. There are many different options, making them great for modern data flow needs.

## Data Processing and Computation

Data processing and computation, also called data ingestion, is the process of 
turning raw data into a format that's easy to work with for analysis and decision-making. 
This includes tasks like cleaning up messy data, combining it, summarizing it, and adding relevant details to make it more meaningful. Modern tools often handle big datasets by 
breaking the work down into pieces and processing them from there. These tools are 
key for tasks like batch processing, real-time data streams, and digging into data to 
find useful insights. This way, companies and people can use the data.

## Workflow and DataOps

Workflow and DataOps encompass the whole process, from data collection to processing, as well as any human contribution points. These tools help move data from one place to another while making sure it's correct and ready to use. They also make it easier for teams to work together by showing how data flows, keeping track of changes, and making sure processes can be repeated. DataOps applies ideas from agile and DevOps to improve how data is handled and shared.

## Data Infrastructure and Monitoring

Data infrastructure is the bones of it all. It includes all of the physical 
and virtual components that support the storage, processing, and movement of data. 
This category covers servers, networking equipment, cloud services, and 
databases that form the backbone of any data  system. Another example of infrastructure would be the highways and other roads of America. Proper data 
infrastructure ensures that data is accessible, secure, and scalable, enabling 
organizations to handle increasing amounts of data efficiently. Monitoring tools 
are also a very importat part of data infrastructure because they help track system 
performance, identify bottlenecks, and ensure reliability. Without a solid data 
infrastructure, even the most advanced data engineering tools would struggle to 
function. 

## ML/AI Platform

ML platforms offer tools for building, training, and deploying machine learning models, covering tasks like data preparation, model training, parameter optimization, and deployment workflows. These platforms often use cloud-based solutions for scalability and support teamwork through features like version control and experiment tracking. Open-source examples include PyTorch and TensorFlow, enabling organizations to make informed, data-driven decisions.

## Metadata Management

Metadata management is the process of organizing metadata to ensure that it is accessible, accurate, and compliant with applicable regulations. Metadata, often referred to as "data about data," provides context and meaning to raw data, making it easier to understand, go through, and utilize. Effective metadata management is crucial in larger databases, where the sheer volume of data can make it challenging to maintain organization and compliance. Tools in this category help catalog, govern, and track metadata, enabling better data discovery, lineage, and quality control.

## Analytics and Visualization

Analytics and visualization tools help make sense of data and share insights. Analytics tools analyze data to find patterns, trends, and useful information. Visualization tools turn this data into charts, graphs, and dashboards that are easy to understand.

Modern platforms often combine analytics and visualization, making it easier to analyze and present data in one place. These tools are used in many areas, like business and research, to find and explain important information. Examples of open-source tools include Apache Superset, Metabase, and Plotly.

# Digging into the details

In the following sections I identify three subcategories of data engineering tools of greatest interest to me.

## Event Hubs

Event Hubs is a data ingestion service by Microsoft Azure. It is designed to be low-latency and has the ability to process millions of events per second, from several sources. 

### Characteristics of Event Hubs

1. **Stream Analytics:**
   - Data is processed in real time. Event Hubs takes in raw streaming data and Stream Analytics looks at it and can make a decision in that moment, like send a notification or log the data.
   - Data can be output to Power BI, Azure Data Lake, Blob Storage, or SQL Database. There are many ways to output this data.

2. **Azure Data Lake & Blob Storage:**
   - Stream Analytics is able to work to store results through Blob Storage or Data Lakes. This is useful for long-term storage, or when something needs to be reported at a later date.
   - Blob storage tends to work better for smaller-scale analytics, as it is simple and cheap. It can also be used to archive or backup.
   - Data Lake is good for large-scale data, 
   especially for advanced analytics. It is hierarchal in structure, unlike blob storage, so it is especially good for those larger real-time cases in terms of relation to Azure Stream Analytics.

### Why It's Important

1. **Real-Time Processing**
   - Because ASA processes data in real time, it can be used for critical events like equipment failures and fraud detection.
   - Businesses, governments, and even international organizations can all use this for scenarios like real-world emergencies on any scale.
2. **Simplicity**
   - There is no need for complex programming languages to use ASA.
   - The queries are akin to SQL, making ASA accessible for more organizations.

### Azure Stream Analytics versus OLTP
   - ASA is built for real-time data analysis, while OLTP is built for real-time business transactions.
   - Generally, OLTP is a little faster, as ASA is often handling larger queries in real time.
   - ASA is not designed to have many users on the system, while OLTP is built with that in mind.
   - ASA does not store anything directly, or change it, but OLTP often will, as it is built to update, store, delete, and even add data.
   - OLTP is used for things like e-commerce checkout and inventory updates.
   - ASA is used for things like detecting fraud and live dashboards displaying real-time data.

### Why Is It Interesting?


## subject2

summary 

### Characteristics of Event Hubs

1. **Stream Analytics:**
   - 

2. **Azure Data Lake & Blob Storage:**
   - 

### Why It's Important

### Azure Stream Analytics versus OLTP

### Why Is It Interesting?

## subject3

summary 

### Characteristics of Event Hubs

1. **Stream Analytics:**
   - 

2. **Azure Data Lake & Blob Storage:**
   - 

### Why It's Important

### Azure Stream Analytics versus OLTP

### Why Is It Interesting?

# Reflection

Convert these questions into brief paragraph responses (two or three sentence).

* what did you like about this project?

* what did you find hardest about this project?

* what did you find most surprising about this assignment?

* how would you approach this project differently next time?


# README

A quality README is an important part of EVERY project. Using the Quarto *include* command we're including a copy of your README in the project report so that a human can evaluate it.

Make sure that you edit the README so that it's explanatory!  Note that you don't need a readme within the *reports* folder for this assignment. We're only
focused on the root *README.md*.

[Here is some info](https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/) on how to write a good README!

::: {style="background:lightgray; margin-left:20px; border-top: 3px solid black; border-bottom: 3px solid black; padding-left:20px; padding-right:20px"}
{{< include ../README.md >}}
:::
